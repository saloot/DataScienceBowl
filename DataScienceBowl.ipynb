{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science Bowl 2019\n\n# Introduction\n\nPBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.\n\n**Where does the data for the competition come from?**\nThe data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.\n\n**What is the PBS KIDS Measure Up! app?**\nIn the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. \n\nBesides the info provided above by Kaggle, I found the following additional info on the website of the app:\n\nSpecific features of Measure Up! include:\n\n* 19 unique measuring games.\n* 10 measurement-focused video clips.\n* Sticker books featuring favorite PBS KIDS characters.\n* Rewards for completion of tasks.\n* Embedded challenges and reports to help parents and caregivers monitor kids’ progress.\n* Ability to track your child's progress using the PBS KIDS Super Vision companion app.\n\n**Evaluation**\nSubmissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\n\nFor each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.\n\nNote that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true."},{"metadata":{},"cell_type":"markdown","source":"# Table of contents\n\n* [1. Understanding the train data](#1.-Understanding-the-train-data)\n* [2. Understanding the test set](#2.-Understanding-the-test-set)\n* [3. Understanding and visualizing the train labels](#3.-Understanding-and-visualizing-the train-labels)\n* [4. Feature engineering](#4.-Feature-engineering)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn.model_selection import GroupKFold\nfrom typing import Any\nfrom numba import jit\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom itertools import product\nimport copy\nimport time\nimport pickle\nimport _pickle as cPickle\nimport random\nseed = 1234\nrandom.seed(seed)\nnp.random.seed(seed)","execution_count":1,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load reduced data from file\nbase_data_folder = '/kaggle/input'\nbase_results_folder = '/kaggle/working'  #'./Data/\ntry:\n    with open(os.path.join(base_results_folder,'train_data.pkl'), 'rb') as fid:\n        reduce_train = pickle.load(fid)\nexcept:\n    reduce_train = None\n    \ntry:\n    with open(os.path.join(base_results_folder,'test_data.pkl'), 'rb') as fid:\n        reduce_test = pickle.load(fid)\nexcept:\n    reduce_test = None\n     \nif not reduce_train:\n    for dirname, _, filenames in os.walk(base_data_folder):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n            file_path = os.path.join(dirname, filename)\n            \n            if filename == 'train.csv':\n                train = pd.read_csv(file_path)\n            elif filename == 'train_labels.csv':\n                train_labels = pd.read_csv(file_path)\n            elif filename == 'test.csv':\n                test = pd.read_csv(file_path)\n            elif filename == 'specs.csv':\n                specs = pd.read_csv(file_path)\n            elif filename == 'sample_submission.csv':\n                sample_submission = pd.read_csv(file_path)","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/data-science-bowl-2019/sample_submission.csv\n/kaggle/input/data-science-bowl-2019/specs.csv\n/kaggle/input/data-science-bowl-2019/train_labels.csv\n/kaggle/input/data-science-bowl-2019/train.csv\n/kaggle/input/data-science-bowl-2019/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Understanding the train data"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 11 million rows and just 11 columns. However, Kaggle provided the following note: Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nAs there is no point in keeping training data that cannot be used for training anyway, I am getting rid of the installation_ids that never took an assessment\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, we have now lost about 3 million rows."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of unique installations in our \"smaller\" train set is now 4242."},{"metadata":{"trusted":false},"cell_type":"code","source":"keep_id.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will now add some new columns based on the timestamp, and visualize these."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain = get_time(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Understanding the test set\n\nFrom Kaggle: For each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"   event_id      game_session                 timestamp  \\\n0  27253bdc  0ea9ecc81a565215  2019-09-10T16:50:24.910Z   \n1  27253bdc  c1ea43d8b8261d27  2019-09-10T16:50:55.503Z   \n2  27253bdc  7ed86c6b72e725e2  2019-09-10T16:51:51.805Z   \n3  27253bdc  7e516ace50e7fe67  2019-09-10T16:53:12.825Z   \n4  7d093bf9  a022c3f60ba547e7  2019-09-10T16:54:12.115Z   \n\n                                          event_data installation_id  \\\n0             {\"event_code\": 2000, \"event_count\": 1}        00abaee7   \n1             {\"event_code\": 2000, \"event_count\": 1}        00abaee7   \n2             {\"event_code\": 2000, \"event_count\": 1}        00abaee7   \n3             {\"event_code\": 2000, \"event_count\": 1}        00abaee7   \n4  {\"version\":\"1.0\",\"round\":0,\"event_count\":1,\"ga...        00abaee7   \n\n   event_count  event_code  game_time                    title  type  \\\n0            1        2000          0  Welcome to Lost Lagoon!  Clip   \n1            1        2000          0     Magma Peak - Level 1  Clip   \n2            1        2000          0     Magma Peak - Level 2  Clip   \n3            1        2000          0  Crystal Caves - Level 1  Clip   \n4            1        2000          0                Chow Time  Game   \n\n          world  \n0          NONE  \n1     MAGMAPEAK  \n2     MAGMAPEAK  \n3  CRYSTALCAVES  \n4  CRYSTALCAVES  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>game_session</th>\n      <th>timestamp</th>\n      <th>event_data</th>\n      <th>installation_id</th>\n      <th>event_count</th>\n      <th>event_code</th>\n      <th>game_time</th>\n      <th>title</th>\n      <th>type</th>\n      <th>world</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>27253bdc</td>\n      <td>0ea9ecc81a565215</td>\n      <td>2019-09-10T16:50:24.910Z</td>\n      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n      <td>00abaee7</td>\n      <td>1</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>Welcome to Lost Lagoon!</td>\n      <td>Clip</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27253bdc</td>\n      <td>c1ea43d8b8261d27</td>\n      <td>2019-09-10T16:50:55.503Z</td>\n      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n      <td>00abaee7</td>\n      <td>1</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>Magma Peak - Level 1</td>\n      <td>Clip</td>\n      <td>MAGMAPEAK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27253bdc</td>\n      <td>7ed86c6b72e725e2</td>\n      <td>2019-09-10T16:51:51.805Z</td>\n      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n      <td>00abaee7</td>\n      <td>1</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>Magma Peak - Level 2</td>\n      <td>Clip</td>\n      <td>MAGMAPEAK</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27253bdc</td>\n      <td>7e516ace50e7fe67</td>\n      <td>2019-09-10T16:53:12.825Z</td>\n      <td>{\"event_code\": 2000, \"event_count\": 1}</td>\n      <td>00abaee7</td>\n      <td>1</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>Crystal Caves - Level 1</td>\n      <td>Clip</td>\n      <td>CRYSTALCAVES</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7d093bf9</td>\n      <td>a022c3f60ba547e7</td>\n      <td>2019-09-10T16:54:12.115Z</td>\n      <td>{\"version\":\"1.0\",\"round\":0,\"event_count\":1,\"ga...</td>\n      <td>00abaee7</td>\n      <td>1</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>Chow Time</td>\n      <td>Game</td>\n      <td>CRYSTALCAVES</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.installation_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 1.1 million rows on a thousand unique installation_ids in the test set. Below, you can see that we have this same amount of rows in the sample submission. This means that there are no installation_ids without assessment in the test set indeed."},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another thing that I would like to check is if there is any overlap with regards to installation_id's in the train and test set. As you can see, there are no installation_id's that appear in both train and test."},{"metadata":{"trusted":false},"cell_type":"code","source":"set(list(train.installation_id.unique())).intersection(set(list(test.installation_id.unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the date ranges?"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"test['timestamp'] = pd.to_datetime(test['timestamp'])\nprint(f'The date range in train is: {train.timestamp.dt.date.min()} to {train.timestamp.dt.date.max()}')\nprint(f'The date range in test is: {test.timestamp.dt.date.min()} to {test.timestamp.dt.date.max()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The date range is more or less the same, so we are talking about a dataset that seems (randomly) split on installation_id. Well actually \"sort of\" as Kaggle seems to have done this on installation_id's with assessments first, and added the \"left-overs\" with no assessments taken to the train set."},{"metadata":{},"cell_type":"markdown","source":"# 3. Understanding and visualizing the train labels"},{"metadata":{},"cell_type":"markdown","source":"The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n\nI started by visualizing some of these columns"},{"metadata":{},"cell_type":"markdown","source":"From Kaggle: The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\nHowever, in the first version I already noticed that I had one attempt too many for this installation_id when mapping the rows with the train_labels for. It turns out that there are in fact also assessment attemps for Bird Measurer with event_code 4100, which should not count (see below). In this case that also makes sense as this installation_id already had a pass on the first attempt"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[(train.event_code == 4100) & (train.installation_id == \"0006a69f\") & (train.title == \"Bird Measurer (Assessment)\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we exclude the Bird Measurer/4100 rows we get the correct match with the numbers in train_labels for this installation_id (4 correct, 12 incorrect)"},{"metadata":{"trusted":false},"cell_type":"code","source":"train[(train.installation_id == \"0006a69f\") & ((train.type == \"Assessment\") & (train.title == 'Bird Measurer (Assessment)') & (train.event_code == 4110) |\n                                               (train.type == \"Assessment\") & (train.title != 'Bird Measurer (Assessment)') & (train.event_code == 4100))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the question arises: Could there be installation_id's who did assessments (we have already taken out the ones who never took one), but without results in the train_labels? As you can see below, yes there are 628 of those."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train.installation_id.isin(train_labels.installation_id.unique())].installation_id.nunique()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"628"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"As we can not train on those installation_id's anyway, I am taking them out of the train set. This reduces our train set further from 8.3 million rows to 7.7 million."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train.installation_id.isin(train_labels.installation_id.unique())]\ntrain.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(7734558, 11)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Basically what we need to do is to compose aggregated features for each session of which we know the train label. Before I get started, I am quickly checking if game_session alone is the unique identifier in train_labels indeed."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"print(f'Number of rows in train_labels: {train_labels.shape[0]}')\nprint(f'Number of unique game_sessions in train_labels: {train_labels.game_session.nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now that we have that confirmed, I my first step was initially to start by looking for values that are always the same for a game_session in the train dataframe. It turns out that the only one is world. I also checked if some of the datetime variables were unique, but this is not always the case (events within a session may cross midnight).\n\nOf course, on Kaggle it is not always necessary to reinvent the wheel. I knew that I would have to iterate over all the rows and add features that only look at what happened up to the moment at which the an installation_id starts a particluar assessment. I found out that Massoud Hosseinali already posted fantastic code on how to do that in this kernel: https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model. Thanks Massoud, and all credit go to you! As Bruno Aquino reused this code and already added some comments, I am actually using his code.\n\nAs I figured out that datetime variables cannot be matched uniquely to the train_labels, I am starting again with a train dataframe as it originally was (except for keeping timestamp as datetime). The huge code chunck below contains the function to generate features for each row in train_labels."},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train.drop(['date', 'month', 'hour', 'dayofweek'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Credits go to Andrew Lukyanenko\nimport pdb\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['TitleEventCode'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['TitleEventCode'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"TitleEventCode\"].unique()).union(test[\"TitleEventCode\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    #print(activities_map)\n    #pdb.set_trace()\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n\ncategoricals = ['session_title']","execution_count":8,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Credits go to Massoud Hosseinali\n\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title] #from Andrew\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            new_features_encoded_keys = {}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # get installation_id for aggregated features\n            features['InstallationId'] = session['installation_id'].iloc[-1] #from Andrew\n            # add title as feature, remembering that title represents the name of the game\n            features['SessionTitle'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['AccumulatedCorrectAttempts'] = accumulated_correct_attempts\n            features['AccumulatedUncorrectAttempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['DurationMean'] = 0\n            else:\n                features['DurationMean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['AccumulatedAccuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['AccuracyGroup'] = 0\n            elif accuracy == 1:\n                features['AccuracyGroup'] = 3\n            elif accuracy == 0.5:\n                features['AccuracyGroup'] = 2\n            else:\n                features['AccuracyGroup'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['AccuracyGroup']] += 1\n            # mean of the all accuracy groups of this player\n            features['AccumulatedAccuracyGroup'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['AccuracyGroup']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['AccumulatedActions'] = accumulated_actions\n            \n            # Encode keys\n            for itm in features:\n                new_key = str(itm)\n                new_key = new_key.replace(' ','')\n                new_key = new_key.replace('_','')\n                new_key = new_key.replace(',','')\n                new_key = new_key.replace('-','')\n                new_features_encoded_keys[new_key] = features[itm]\n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(new_features_encoded_keys)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(new_features_encoded_keys)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if test_set=True, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in train_set, all assessments are kept\n    return all_assessments","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make this a little bit easier to understand, I am first using the function on one installation_id as an example (same one as used as an example before). Below, I have only displayed the last bunch of columns of the resulting dataframe. As you can see, five rows have been created for this installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_id = train[train.installation_id == \"0006a69f\"]\nsample_id_data = get_data(sample_id) #returns a list\nsample_df = pd.DataFrame(sample_id_data)\nsample_df.iloc[:,-10:]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"   AccumulatedUncorrectAttempts  DurationMean  AccumulatedAccuracy  \\\n0                             0          0.00                  0.0   \n1                             0         39.00                  1.0   \n2                            11         65.50                  0.5   \n3                            11         41.25                  0.5   \n4                            12         39.20                  0.5   \n\n   AccuracyGroup  0  1  2  3  AccumulatedAccuracyGroup  AccumulatedActions  \n0              3  0  0  0  0                       0.0                 647  \n1              0  0  0  0  1                       3.0                1143  \n2              3  1  0  0  1                       1.5                1230  \n3              2  2  0  0  2                       1.5                2159  \n4              3  2  0  1  2                       1.6                2586  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AccumulatedUncorrectAttempts</th>\n      <th>DurationMean</th>\n      <th>AccumulatedAccuracy</th>\n      <th>AccuracyGroup</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>AccumulatedAccuracyGroup</th>\n      <th>AccumulatedActions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>39.00</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11</td>\n      <td>65.50</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>41.25</td>\n      <td>0.5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.5</td>\n      <td>2159</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>39.20</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.6</td>\n      <td>2586</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"When we compare this to the train_labels, you can see that the accuracy_group values are the same so features have been added for all game_session id's. However more importantly, by comparing the accumulated_uncorrect_attempts with num_incorrect, you will see that **only activities before the start of that particular session have been accumulated**."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels[train_labels.installation_id == \"0006a69f\"].iloc[:, [0, 1, -3, -1]]","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"       game_session installation_id  num_incorrect  accuracy_group\n0  6bdf9623adc94d89        0006a69f              0               3\n1  77b8ee947eb84b4e        0006a69f             11               0\n2  901acc108f55a5a1        0006a69f              0               3\n3  9501794defd84e4d        0006a69f              1               2\n4  a9ef3ecb3d1acc6a        0006a69f              0               3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>game_session</th>\n      <th>installation_id</th>\n      <th>num_incorrect</th>\n      <th>accuracy_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6bdf9623adc94d89</td>\n      <td>0006a69f</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>77b8ee947eb84b4e</td>\n      <td>0006a69f</td>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>901acc108f55a5a1</td>\n      <td>0006a69f</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9501794defd84e4d</td>\n      <td>0006a69f</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a9ef3ecb3d1acc6a</td>\n      <td>0006a69f</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In the code below the function is applied to each installation_id in the train dataset.\n\nCompared to the original code I changed the total. In the original code this was set at 17,000. However, since I reduced the train dataframe, I only have 3614 of those left (train.installation_id.nunique()=3614). In addition, I had issues with incorrect rendering of the tdqm bar, and solved this by adding position=0. I also turns out that possible to add a description, which is nice to have."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Credits go to Massoud Hosseinali\n\n#The get_data function is applied to each installation_id and added to the compile_data list\ncompiled_data = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=train.installation_id.nunique(), desc='Installation_id', position=0):\n    # user_sample is a DataFrame that contains only one installation_id\n    compiled_data += get_data(user_sample)","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Installation_id', max=3614, style=ProgressStyle(description_w…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4124b7c9d6b49e0a7c99542e870c74d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Credits go to Massoud Hosseinali\n\n#Compiled_data is converted into a DataFrame and deleted to save memmory\nreduce_train = pd.DataFrame(compiled_data)\ndel compiled_data\nreduce_train.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(17690, 103)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"   Clip  Activity  Assessment  Game  BirdMeasurer(Assessment)  \\\n0    11         3           0     4                         0   \n1    14         4           1     6                         0   \n2    14         4           2     6                         0   \n3    24         9           4    10                         0   \n4    28        10           5    13                         0   \n\n   BottleFiller(Activity)  Rulers  TreasureMap  MagmaPeakLevel1  \\\n0                       0       0            0                0   \n1                       0       0            0                0   \n2                       0       0            0                0   \n3                     165       0            0                0   \n4                     165       0            0                0   \n\n   CrystalCavesLevel3  AllStarSorting  12Monkeys  TreeTopCityLevel3  \\\n0                   0             163          0                  0   \n1                   0             163          0                  0   \n2                   0             163          0                  0   \n3                   0             229          0                  0   \n4                   0             229          0                  0   \n\n   ScrubADub  LiftingHeavyThings  EggDropper(Activity)  OrderingSpheres  \\\n0        115                   0                     0                0   \n1        115                   0                     0                0   \n2        115                   0                     0                0   \n3        195                   0                     0                0   \n4        195                   0                     0                0   \n\n   HappyCamel  FlowerWaterer(Activity)  WateringHole(Activity)  \\\n0           0                      164                       0   \n1           0                      164                       0   \n2           0                      164                       0   \n3           0                      254                      80   \n4           0                      254                      80   \n\n   BugMeasurer(Activity)  SandcastleBuilder(Activity)  Fireworks(Activity)  \\\n0                      0                           89                   91   \n1                    104                           89                   91   \n2                    104                           89                   91   \n3                    104                          126                 1611   \n4                    184                          126                 1611   \n\n   HoneyCake  CartBalancer(Assessment)  BubbleBath  Pirate'sTale  \\\n0          0                         0           0             0   \n1          0                         0           0             0   \n2          0                         0           0             0   \n3          0                         0         133             0   \n4          0                         0         133             0   \n\n   MagmaPeakLevel2  AirShow  PanBalance  CrystalsRule  BalancingAct  \\\n0                0        0           0             0             0   \n1                0      193           0            78             0   \n2                0      193           0            78             0   \n3                0      193           0            78             0   \n4                0      336           0           310             0   \n\n   CrystalCavesLevel1  HeavyHeavierHeaviest  ChickenBalancer(Activity)  \\\n0                   0                     0                          0   \n1                   0                     0                          0   \n2                   0                     0                          0   \n3                   0                     0                          0   \n4                   0                     0                          0   \n\n   CauldronFiller(Assessment)  MushroomSorter(Assessment)  CrystalCavesLevel2  \\\n0                           0                           0                   0   \n1                           0                           0                   0   \n2                           0                           0                   0   \n3                           0                           0                   0   \n4                           0                           0                   0   \n\n   TreeTopCityLevel1  ChowTime  TreeTopCityLevel2  ChestSorter(Assessment)  \\\n0                  0         0                  0                        0   \n1                  0         0                  0                        0   \n2                  0         0                  0                        0   \n3                  0         0                  0                        0   \n4                  0         0                  0                        0   \n\n   SlopProblem  WelcometoLostLagoon!  CostumeBox  LeafLeader  DinoDrink  \\\n0            0                     0           0           0          0   \n1            0                     0           0           0          0   \n2            0                     0           0           0          0   \n3            0                     0           0           0        110   \n4            0                     0           0           0        110   \n\n   DinoDive  2050  4100  4230  5000  4235  2060  4110  5010  2070  2075  2080  \\\n0         0     6     0     0     0     0     0     0     0     0     0     4   \n1         0     6     5     0     0     0     1     2     0     1     0     4   \n2         0     6     5     0     0     0     1    13     0     1     0     4   \n3         0     9     6     0     5     0     2    13     5     2     0     8   \n4         0     9    12     0     5     0     3    13     5     2     1     8   \n\n   2081  2083  3110  4010  3120  3121  4020  4021  4022  4025  4030  4031  \\\n0     1     2    77     4     7     9    92    14    31    19   121     0   \n1     1     2   223     6    11    16   127    14    31    37   149     0   \n2     1     2   225     6    22    16   127    14    31    59   171     0   \n3     2     5   336    10    25    40   243    29    45    93   314     6   \n4     2     5   457    12    30    53   277    29    45   105   331     6   \n\n   3010  4035  4040  3020  3021  4045  2000  4050  2010  2020  4070  2025  \\\n0    79     1     0     7     9     0    18     0     0    20    94     4   \n1   226     6     2    11    16     0    25     0     1    26   156     5   \n2   228     6     2    22    16     0    26     0     1    27   160     5   \n3   341    14     9    25    40     2    47     0     2    52   348     9   \n4   463    15    10    30    53     2    56     0     3    64   387    10   \n\n   2030  4080  2035  2040  4090  4220  4095 InstallationId  SessionTitle  \\\n0    18     0     0     6     4     0     0       0006a69f            32   \n1    22     0     1     6     4     0     0       0006a69f             0   \n2    22     0     1     6     4     0     0       0006a69f            32   \n3    43     0     5    10     4     9     1       0006a69f            32   \n4    53     0     6    10     4     9     1       0006a69f             0   \n\n   AccumulatedCorrectAttempts  AccumulatedUncorrectAttempts  DurationMean  \\\n0                           0                             0          0.00   \n1                           1                             0         39.00   \n2                           1                            11         65.50   \n3                           2                            11         41.25   \n4                           3                            12         39.20   \n\n   AccumulatedAccuracy  AccuracyGroup  0  1  2  3  AccumulatedAccuracyGroup  \\\n0                  0.0              3  0  0  0  0                       0.0   \n1                  1.0              0  0  0  0  1                       3.0   \n2                  0.5              3  1  0  0  1                       1.5   \n3                  0.5              2  2  0  0  2                       1.5   \n4                  0.5              3  2  0  1  2                       1.6   \n\n   AccumulatedActions  \n0                 647  \n1                1143  \n2                1230  \n3                2159  \n4                2586  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Clip</th>\n      <th>Activity</th>\n      <th>Assessment</th>\n      <th>Game</th>\n      <th>BirdMeasurer(Assessment)</th>\n      <th>BottleFiller(Activity)</th>\n      <th>Rulers</th>\n      <th>TreasureMap</th>\n      <th>MagmaPeakLevel1</th>\n      <th>CrystalCavesLevel3</th>\n      <th>AllStarSorting</th>\n      <th>12Monkeys</th>\n      <th>TreeTopCityLevel3</th>\n      <th>ScrubADub</th>\n      <th>LiftingHeavyThings</th>\n      <th>EggDropper(Activity)</th>\n      <th>OrderingSpheres</th>\n      <th>HappyCamel</th>\n      <th>FlowerWaterer(Activity)</th>\n      <th>WateringHole(Activity)</th>\n      <th>BugMeasurer(Activity)</th>\n      <th>SandcastleBuilder(Activity)</th>\n      <th>Fireworks(Activity)</th>\n      <th>HoneyCake</th>\n      <th>CartBalancer(Assessment)</th>\n      <th>BubbleBath</th>\n      <th>Pirate'sTale</th>\n      <th>MagmaPeakLevel2</th>\n      <th>AirShow</th>\n      <th>PanBalance</th>\n      <th>CrystalsRule</th>\n      <th>BalancingAct</th>\n      <th>CrystalCavesLevel1</th>\n      <th>HeavyHeavierHeaviest</th>\n      <th>ChickenBalancer(Activity)</th>\n      <th>CauldronFiller(Assessment)</th>\n      <th>MushroomSorter(Assessment)</th>\n      <th>CrystalCavesLevel2</th>\n      <th>TreeTopCityLevel1</th>\n      <th>ChowTime</th>\n      <th>TreeTopCityLevel2</th>\n      <th>ChestSorter(Assessment)</th>\n      <th>SlopProblem</th>\n      <th>WelcometoLostLagoon!</th>\n      <th>CostumeBox</th>\n      <th>LeafLeader</th>\n      <th>DinoDrink</th>\n      <th>DinoDive</th>\n      <th>2050</th>\n      <th>4100</th>\n      <th>4230</th>\n      <th>5000</th>\n      <th>4235</th>\n      <th>2060</th>\n      <th>4110</th>\n      <th>5010</th>\n      <th>2070</th>\n      <th>2075</th>\n      <th>2080</th>\n      <th>2081</th>\n      <th>2083</th>\n      <th>3110</th>\n      <th>4010</th>\n      <th>3120</th>\n      <th>3121</th>\n      <th>4020</th>\n      <th>4021</th>\n      <th>4022</th>\n      <th>4025</th>\n      <th>4030</th>\n      <th>4031</th>\n      <th>3010</th>\n      <th>4035</th>\n      <th>4040</th>\n      <th>3020</th>\n      <th>3021</th>\n      <th>4045</th>\n      <th>2000</th>\n      <th>4050</th>\n      <th>2010</th>\n      <th>2020</th>\n      <th>4070</th>\n      <th>2025</th>\n      <th>2030</th>\n      <th>4080</th>\n      <th>2035</th>\n      <th>2040</th>\n      <th>4090</th>\n      <th>4220</th>\n      <th>4095</th>\n      <th>InstallationId</th>\n      <th>SessionTitle</th>\n      <th>AccumulatedCorrectAttempts</th>\n      <th>AccumulatedUncorrectAttempts</th>\n      <th>DurationMean</th>\n      <th>AccumulatedAccuracy</th>\n      <th>AccuracyGroup</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>AccumulatedAccuracyGroup</th>\n      <th>AccumulatedActions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>163</td>\n      <td>0</td>\n      <td>0</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>164</td>\n      <td>0</td>\n      <td>0</td>\n      <td>89</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>77</td>\n      <td>4</td>\n      <td>7</td>\n      <td>9</td>\n      <td>92</td>\n      <td>14</td>\n      <td>31</td>\n      <td>19</td>\n      <td>121</td>\n      <td>0</td>\n      <td>79</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>9</td>\n      <td>0</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>94</td>\n      <td>4</td>\n      <td>18</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0006a69f</td>\n      <td>32</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>4</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>163</td>\n      <td>0</td>\n      <td>0</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>164</td>\n      <td>0</td>\n      <td>104</td>\n      <td>89</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>193</td>\n      <td>0</td>\n      <td>78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>223</td>\n      <td>6</td>\n      <td>11</td>\n      <td>16</td>\n      <td>127</td>\n      <td>14</td>\n      <td>31</td>\n      <td>37</td>\n      <td>149</td>\n      <td>0</td>\n      <td>226</td>\n      <td>6</td>\n      <td>2</td>\n      <td>11</td>\n      <td>16</td>\n      <td>0</td>\n      <td>25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26</td>\n      <td>156</td>\n      <td>5</td>\n      <td>22</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0006a69f</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>39.00</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>4</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>163</td>\n      <td>0</td>\n      <td>0</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>164</td>\n      <td>0</td>\n      <td>104</td>\n      <td>89</td>\n      <td>91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>193</td>\n      <td>0</td>\n      <td>78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>225</td>\n      <td>6</td>\n      <td>22</td>\n      <td>16</td>\n      <td>127</td>\n      <td>14</td>\n      <td>31</td>\n      <td>59</td>\n      <td>171</td>\n      <td>0</td>\n      <td>228</td>\n      <td>6</td>\n      <td>2</td>\n      <td>22</td>\n      <td>16</td>\n      <td>0</td>\n      <td>26</td>\n      <td>0</td>\n      <td>1</td>\n      <td>27</td>\n      <td>160</td>\n      <td>5</td>\n      <td>22</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0006a69f</td>\n      <td>32</td>\n      <td>1</td>\n      <td>11</td>\n      <td>65.50</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1230</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>24</td>\n      <td>9</td>\n      <td>4</td>\n      <td>10</td>\n      <td>0</td>\n      <td>165</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>229</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>254</td>\n      <td>80</td>\n      <td>104</td>\n      <td>126</td>\n      <td>1611</td>\n      <td>0</td>\n      <td>0</td>\n      <td>133</td>\n      <td>0</td>\n      <td>0</td>\n      <td>193</td>\n      <td>0</td>\n      <td>78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>110</td>\n      <td>0</td>\n      <td>9</td>\n      <td>6</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2</td>\n      <td>13</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>5</td>\n      <td>336</td>\n      <td>10</td>\n      <td>25</td>\n      <td>40</td>\n      <td>243</td>\n      <td>29</td>\n      <td>45</td>\n      <td>93</td>\n      <td>314</td>\n      <td>6</td>\n      <td>341</td>\n      <td>14</td>\n      <td>9</td>\n      <td>25</td>\n      <td>40</td>\n      <td>2</td>\n      <td>47</td>\n      <td>0</td>\n      <td>2</td>\n      <td>52</td>\n      <td>348</td>\n      <td>9</td>\n      <td>43</td>\n      <td>0</td>\n      <td>5</td>\n      <td>10</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0006a69f</td>\n      <td>32</td>\n      <td>2</td>\n      <td>11</td>\n      <td>41.25</td>\n      <td>0.5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.5</td>\n      <td>2159</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>10</td>\n      <td>5</td>\n      <td>13</td>\n      <td>0</td>\n      <td>165</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>229</td>\n      <td>0</td>\n      <td>0</td>\n      <td>195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>254</td>\n      <td>80</td>\n      <td>184</td>\n      <td>126</td>\n      <td>1611</td>\n      <td>0</td>\n      <td>0</td>\n      <td>133</td>\n      <td>0</td>\n      <td>0</td>\n      <td>336</td>\n      <td>0</td>\n      <td>310</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>110</td>\n      <td>0</td>\n      <td>9</td>\n      <td>12</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>13</td>\n      <td>5</td>\n      <td>2</td>\n      <td>1</td>\n      <td>8</td>\n      <td>2</td>\n      <td>5</td>\n      <td>457</td>\n      <td>12</td>\n      <td>30</td>\n      <td>53</td>\n      <td>277</td>\n      <td>29</td>\n      <td>45</td>\n      <td>105</td>\n      <td>331</td>\n      <td>6</td>\n      <td>463</td>\n      <td>15</td>\n      <td>10</td>\n      <td>30</td>\n      <td>53</td>\n      <td>2</td>\n      <td>56</td>\n      <td>0</td>\n      <td>3</td>\n      <td>64</td>\n      <td>387</td>\n      <td>10</td>\n      <td>53</td>\n      <td>0</td>\n      <td>6</td>\n      <td>10</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0006a69f</td>\n      <td>0</td>\n      <td>3</td>\n      <td>12</td>\n      <td>39.20</td>\n      <td>0.5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.6</td>\n      <td>2586</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now, we need to do the same thing for the test set. Parameter test_set=True leads to accuracy_group=0 and only the last assessment is kept (so only one row per installation_id)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"new_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=test.installation_id.nunique(), desc='Installation_id', position=0):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nreduce_test = pd.DataFrame(new_test)","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Installation_id', max=1000, style=ProgressStyle(description_w…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ace46ba9e48410fb7c12f427bfea6a9"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the training data into a picke\nbase_results_folder ='/kaggle/working'\nwith open(os.path.join(base_results_folder,'train_data.pkl'), 'wb') as fid:\n    pickle.dump(reduce_train, fid)\n\nwith open(os.path.join(base_results_folder,'test_data.pkl'), 'wb') as fid:\n    pickle.dump(reduce_test, fid)","execution_count":17,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pickle' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b82f6c252620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_results_folder\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/working'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_results_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_results_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"]}]},{"metadata":{"trusted":false},"cell_type":"code","source":"reduce_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reduce_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Baseline Model\n\nIn this competition,regression with rounding of coefficients is clearly the way to go as explained by Andrew Lukyanenko in his excellent kernel: https://www.kaggle.com/artgor/quick-and-dirty-regression\n\nStep 1: Just get it working....\n\nCredits for this section go to Andrew. I have only made small changes, and are basically just using other (less actually) features. As I was just focused on getting it to work, I changed the test and train set into the names that Andrew uses (reduce_train and reduce_test)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['InstallationSessionCount'] = df.groupby(['InstallationId'])['Clip'].transform('count')\n        df['InstallationDurationMean'] = df.groupby(['InstallationId'])['DurationMean'].transform('mean')\n        df['InstallationTitleNunique'] = df.groupby(['InstallationId'])['SessionTitle'].transform('nunique')\n        \n        df['SumEventCodeCount'] = df[['2050', '4100', '4230', '5000', '4235', '2060', '4110', '5010', '2070', '2075',\n                                      '2080', '2081', '2083', '3110', '4010', '3120', '3121', '4020', '4021', \n                                      '4022', '4025', '4030', '4031', '3010', '4035', '4040', '3020', '3021', \n                                      '4045', '2000', '4050', '2010', '2020', '4070', '2025', '2030', '4080', \n                                      '2035', '2040', '4090', '4220', '4095']].sum(axis = 1)\n        \n        df['InstallationEventCodeCountMean'] = df.groupby(['InstallationId'])['SumEventCodeCount'].transform('mean')\n        \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    features = [x for x in features if x not in ['AccuracyGroup', 'InstallationId']] + ['acc_' + title for title in assess_titles]\n   \n    return reduce_train, reduce_test, features\n# call feature engineering function\nreduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n             'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'EarlyStoppingRounds': 100, 'eval_metric': 'cappa'\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = reduce_train['AccuracyGroup']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GroupKFold is chosen for cross validation as we want all sessions of an installation_id to end up in either train or valid. See also the RegressorModel class."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = GroupKFold(n_splits=n_fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['GameSession', 'InstallationId', 'timestamp', 'AccuracyGroup', 'timestampDate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quadratic Weighted Cappa function. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    y_pred[y_pred <= 1.12232214] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n    y_pred[y_pred > 2.22506454] = 3\n\n    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n\n    return 'cappa', qwk(y_true, y_pred), True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model wrappers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class LGBWrapper_regr(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb_regr\n        else:\n            eval_metric = 'auc'\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['EarlyStoppingRounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n\n    \ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['EarlyStoppingRounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n\n\nclass CatWrapper(object):\n    \"\"\"\n    A wrapper for catboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = cat.CatBoostClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**{k: v for k, v in params.items() if k != 'cat_cols'})\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = None\n        else:\n            categorical_columns = None\n        \n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set,\n                       verbose=params['verbose'], early_stopping_rounds=params['EarlyStoppingRounds'],\n                       cat_features=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if 'MultiClass' not in self.model.get_param('loss_function'):\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)\n\n\nclass XGBWrapper(object):\n    \"\"\"\n    A wrapper for xgboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = xgb.XGBClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_metric=eval_qwk_xgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['EarlyStoppingRounds'])\n\n        scores = self.model.evals_result()\n        self.best_score_ = {k: {m: m_v[-1] for m, m_v in v.items()} for k, v in scores.items()}\n        self.best_score_ = {k: {m: n if m != 'cappa' else -n for m, n in v.items()} for k, v in self.best_score_.items()}\n\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MainTransformer"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}Int{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FeatureTransformer."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n#                          or 'attempt' in col]\n        \n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RegressorModel class."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class RegressorModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='rmse',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['InstallationId'])):\n\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            # print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=20)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CategoricalTransformer class."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from category_encoders.ordinal import OrdinalEncoder\n\nclass CategoricalTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cat_cols=None, drop_original: bool = False, encoder=OrdinalEncoder()):\n        \"\"\"\n        Categorical transformer. This is a wrapper for categorical encoders.\n\n        :param cat_cols:\n        :param drop_original:\n        :param encoder:\n        \"\"\"\n        self.cat_cols = cat_cols\n        self.drop_original = drop_original\n        self.encoder = encoder\n        self.default_encoder = OrdinalEncoder()\n\n    def fit(self, X, y=None):\n\n        if self.cat_cols is None:\n            kinds = np.array([dt.kind for dt in X.dtypes])\n            is_cat = kinds == 'O'\n            self.cat_cols = list(X.columns[is_cat])\n        self.encoder.set_params(cols=self.cat_cols)\n        self.default_encoder.set_params(cols=self.cat_cols)\n\n        self.encoder.fit(X[self.cat_cols], y)\n        self.default_encoder.fit(X[self.cat_cols], y)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        new_cat_names = [f'{col}Encoded' for col in self.cat_cols]\n        encoded_data = self.encoder.transform(data[self.cat_cols])\n        if encoded_data.shape[1] == len(self.cat_cols):\n            data[new_cat_names] = encoded_data\n        else:\n            pass\n\n        if self.drop_original:\n            data = data.drop(self.cat_cols, axis=1)\n        else:\n            data[self.cat_cols] = self.default_encoder.transform(data[self.cat_cols])\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}\nregressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\nregressor_model1.fit(X=reduce_train, y=y, folds=folds, params=params, preprocesser=mt, transformers=transformers,\n                    eval_metric='cappa', cols_to_drop=cols_to_drop)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The OptimizedRounder class."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pr1 = regressor_model1.predict(reduce_train)\n\noptR = OptimizedRounder()\noptR.fit(pr1.reshape(-1,), y)\ncoefficients = optR.coefficients()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"coefficients","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\nqwk(y, opt_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# using the optimized coefficients to round our test predictions\npr1 = regressor_model1.predict(reduce_test)\npr1[pr1 <= coefficients[0]] = 0\npr1[np.where(np.logical_and(pr1 > coefficients[0], pr1 <= coefficients[1]))] = 1\npr1[np.where(np.logical_and(pr1 > coefficients[1], pr1 <= coefficients[2]))] = 2\npr1[pr1 > coefficients[2]] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission['accuracy_group'] = pr1.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"021ac187d65f4b2f971735528f65ef56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"104b8cea5efe463e824ab97a1f536b70":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ed92890f77145b79f8f7dbbbc77e1d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3149b7e6bbc6491c993f977972186290":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_443a547b540c4d3bb3bde37e4c5627e0","placeholder":"​","style":"IPY_MODEL_e947c755dd6f4649ba186e949952c014","value":" 3614/3614 [05:28&lt;00:00, 11.01it/s]"}},"443a547b540c4d3bb3bde37e4c5627e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5403d367254c4520ad67bbd65b6b90a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Installation_id: 100%","description_tooltip":null,"layout":"IPY_MODEL_1ed92890f77145b79f8f7dbbbc77e1d9","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee122d98d107453e91b35340b4dbb87a","value":1000}},"5ab8f62cf9954e1194194d69b101cf5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"5dd804de5e0e4af4b6afd2ceaa24f1fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f94da66be6a42daad6abc107e5a0cbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"853ea8bd13d94cf281313db5ae633e96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f94da66be6a42daad6abc107e5a0cbc","placeholder":"​","style":"IPY_MODEL_5dd804de5e0e4af4b6afd2ceaa24f1fc","value":" 1000/1000 [00:48&lt;00:00, 20.50it/s]"}},"9adb1b1557654dbe8eff694364c195f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5403d367254c4520ad67bbd65b6b90a1","IPY_MODEL_853ea8bd13d94cf281313db5ae633e96"],"layout":"IPY_MODEL_104b8cea5efe463e824ab97a1f536b70"}},"bec0bfbcee9a43299247ca605a6581bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1c0feddb7084ee6942e7098e9169705","IPY_MODEL_3149b7e6bbc6491c993f977972186290"],"layout":"IPY_MODEL_cc1084b397bd4a93972a8ba1821d44ca"}},"c1c0feddb7084ee6942e7098e9169705":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Installation_id: 100%","description_tooltip":null,"layout":"IPY_MODEL_021ac187d65f4b2f971735528f65ef56","max":3614,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5ab8f62cf9954e1194194d69b101cf5c","value":3614}},"cc1084b397bd4a93972a8ba1821d44ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e947c755dd6f4649ba186e949952c014":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee122d98d107453e91b35340b4dbb87a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}